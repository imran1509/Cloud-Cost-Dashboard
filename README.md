# Cloud-Cost-Dashboard
# Cloud Cost Monitoring with Steampipe, PostgreSQL, and Grafana

## ğŸ“Œ Overview
This project provides a free and open-source way to monitor AWS cloud costs (GCP and Azure coming soon) using:

- **Steampipe**: For querying cloud APIs using SQL.
- **PostgreSQL**: For storing query results.
- **Grafana**: For visualizing costs in a dashboard.

You can easily extend this to monitor GCP and Azure costs as well.

---

## âœ… Features
- Unified dashboard for all cloud costs.
- No vendor lock-in.
- No cost (uses free-tier APIs and open-source tools).
- Easily customizable with SQL.

---

## ğŸ”§ Prerequisites
Before starting, ensure you have the following tools installed and configured:

### 1. AWS Credentials
You need an AWS account with programmatic access set up. Store credentials as environment variables:
```bash
export AWS_ACCESS_KEY_ID=your_access_key_id
export AWS_SECRET_ACCESS_KEY=your_secret_access_key
export AWS_DEFAULT_REGION=us-east-1  # or your preferred region
```
Alternatively, configure them using the AWS CLI:
```bash
aws configure
```
> :bulb: To see how to setup IAM, Policy and AWS access. see here [docs/setup-aws-programmatic-access.md](https://github.com/imran1509/Cloud-Cost-Dashboard/blob/main/docs/setup-aws-programmatic-access.md)

### 2. Installed Tools
You must have the following installed:

- [Steampipe](https://steampipe.io/downloads): Query cloud services using SQL.
- [PostgreSQL](https://www.postgresql.org/download/): Used for storing cost data.
- [Grafana](https://grafana.com/grafana/download): Dashboard visualization.
- [Python 3](https://www.python.org/downloads/) (for automation script).

Optional (but recommended):
- Docker & Docker Compose (to simplify deployment).

---

## ğŸ“ Repository Structure
```
cloud-cost-dashboard/
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ aws_cloud_cost_dashboard.json     # Grafana dashboard JSON
â”œâ”€â”€ queries/
â”‚   â””â”€â”€ aws_cost_queries.sql              # SQL queries for AWS billing
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ insert_to_postgres.py             # Python script to insert data
â”œâ”€â”€ .env                                  # Environment variables
â””â”€â”€ README.md                             # This file
```

---

## ğŸ› ï¸ Setup Steps

### Step 1: Clone the Repository
```bash
git clone https://github.com/your-username/cloud-cost-dashboard.git
cd cloud-cost-dashboard
```

### Step 2: Configure AWS Credentials
Ensure your environment is configured with AWS credentials (see Prerequisites).

### Step 3: Install Steampipe & AWS Plugin
```bash
# Update system packages
sudo apt update

# Install dependencies
sudo apt install -y wget gnupg lsb-release

# Add the Steampipe GPG key
wget -qO - https://steampipe.io/signing-key.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/steampipe-archive-keyring.gpg > /dev/null

# Add the Steampipe APT repository
echo "deb [signed-by=/usr/share/keyrings/steampipe-archive-keyring.gpg] https://apt.steampipe.io stable main" | sudo tee /etc/apt/sources.list.d/steampipe.list

# Update again and install Steampipe
sudo apt update
sudo apt install steampipe

# Install the AWS plugin
steampipe plugin install aws
```
Test a query:
```bash
steampipe query
> select account_id, service, sum(unblended_cost) as cost from aws_billing_usage_by_service where usage_start_date >= current_date - interval '30 days' group by account_id, service;
```

### Step 4: Set Up PostgreSQL
Install PostgreSQL and create a new database:
```bash
createdb cloud_costs
```
Set credentials in `.env` file.

### Step 5: Run the Python Script
Install dependencies and run the script to insert AWS cost data:
```bash
pip install psycopg2 python-dotenv
python scripts/insert_to_postgres.py
```
This fetches cost data using Steampipe and inserts it into PostgreSQL.

### Step 6: Import Dashboard to Grafana
1. Start Grafana.
2. Import the JSON from `dashboards/aws_cloud_cost_dashboard.json`.
3. Set PostgreSQL as the data source.

Done! ğŸ‰ You should now see your AWS costs visualized.

---

## ğŸ§  What Each Component Does
- **Steampipe**: Pulls AWS billing and usage data with SQL.
- **Python Script**: Automates inserting that data into a structured DB.
- **PostgreSQL**: Stores cost history for long-term tracking.
- **Grafana**: Visualizes cost per service, region, etc.

---

## ğŸ“Œ To Do / Coming Next
- Add support for GCP and Azure.
- Create Docker Compose setup.
- Add email/Slack alerts for cost thresholds.
- Add cron automation script.

---

## ğŸ¤ Contributing
PRs welcome! Open an issue if you need support or want to contribute queries for other clouds.
